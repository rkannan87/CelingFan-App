@startuml
# Job Queue-Based Retry Handling - Sequence Diagrams
## Case 1: Successful API Call (Happy Path)
```mermaid
participant "Client Application" as Client
participant "API Service" as API
participant "Logging Service" as Logger
Client ->> API: Make API Call
activate API
API --> Client: ‚úì Success (200)
deactivate API
Client ->> Logger: Log Success Metric
Note over Client,Logger: Request completed successfully
```
---
## Case 2: Transient Failure ‚Üí Retry ‚Üí Success
```mermaid
participant "Client Application" as Client
participant "Retry Handler" as RH
participant "API Service" as API
participant "Notification Service" as NS
participant "Logging Service" as Logger
Client ->> RH: Make API Call (Attempt 1)
RH ->> API: Call API
activate API
API --> RH: ‚úó Failure (503)
deactivate API
Note over RH: Exponential Backoff: 1s
RH ->> API: Retry Attempt 2
activate API
API --> RH: ‚úó Failure (502)
deactivate API
Note over RH: Exponential Backoff: 2s
RH ->> API: Retry Attempt 3
activate API
API --> RH: ‚úì Success (200)
deactivate API
RH --> Client: Return Success
RH ->> Logger: Log: "Succeeded after 3 attempts"
RH ->> NS: Send Recovery Alert
NS --> RH: Alert Sent
Note over Client,Logger: Recovered from transient failure
```
---
## Case 3: Initial Retries Exhausted ‚Üí Queue for Later
```mermaid
participant "Client Application" as Client
participant "Retry Handler" as RH
participant "API Service" as API
participant "Job Queue" as Queue
participant "Notification Service" as NS
participant "Retry Worker" as Worker
Client ->> RH: Make API Call (Attempt 1)
RH ->> API: Call API
API --> RH: ‚úó Failure (503)
Note over RH: Exponential Backoff
RH ->> API: Attempt 2
API --> RH: ‚úó Failure (503)
Note over RH: Exponential Backoff
RH ->> API: Attempt 3
API --> RH: ‚úó Failure (503)
Note over RH: Initial retries exhausted
RH ->> Queue: Publish Message<br/>{requestId, payload,<br/>attempt: 3, delay: 3min}
RH ->> NS: Email: "Initial retries failed.<br/>Scheduled for retry in 3 mins"
RH --> Client: Return Queued Status
Note over Queue: Wait 3 minutes<br/>(visibility timeout)
Queue ->> Worker: Deliver Message
Worker ->> API: Attempt 4
API --> Worker: ‚úó Failure (504)
Worker ->> API: Attempt 5
API --> Worker: ‚úó Failure (504)
Worker ->> API: Attempt 6
API --> Worker: ‚úó Failure (504)
Worker ->> Queue: Republish with 2-hour delay<br/>{requestId, attempt: 6,<br/>nextRetry: +2hours}
Worker ->> NS: Email: "Still failing.<br/>Next retry in 2 hours"
Note over Queue,Worker: Continues with 2-hour intervals
```
---
## Case 4: Long-term Retry ‚Üí Eventually Succeeds
```mermaid
participant "Job Queue" as Queue
participant "Retry Worker" as Worker
participant "API Service" as API
participant "Notification Service" as NS
participant "Logging Service" as Logger
Note over Queue: 2-hour retry cycle begins
Queue ->> Worker: Deliver Message (after 2 hours)
Worker ->> API: Attempt 7
API --> Worker: ‚úó Failure
Worker ->> API: Attempt 8
API --> Worker: ‚úó Failure
Worker ->> API: Attempt 9
API --> Worker: ‚úó Failure
Worker ->> Queue: Re-queue for another 2 hours
Note over Queue: Wait 2 hours
Queue ->> Worker: Deliver Message
Worker ->> API: Attempt 10
API --> Worker: ‚úó Failure
Worker ->> API: Attempt 11
API --> Worker: ‚úó Failure
Worker ->> API: Attempt 12
activate API
API --> Worker: ‚úì Success (200)
deactivate API
Worker ->> Queue: Remove from Queue
Worker ->> NS: Email: "API call succeeded<br/>after 12 attempts over 4+ hours"
Worker ->> Logger: Log Recovery Metrics<br/>{totalAttempts: 12,<br/>duration: 4+ hours}
Note over Worker,Logger: Successfully recovered<br/>after extended retry period
```
---
## Case 5: Permanent Failure ‚Üí Dead Letter Queue
```mermaid
participant "Job Queue" as Queue
participant "Retry Worker" as Worker
participant "API Service" as API
participant "Dead Letter Queue" as DLQ
participant "Notification Service" as NS
participant "Incident Management" as IMS
participant "Engineering Team" as Eng
Note over Queue: 2-hour retry cycles continue
loop Retry Attempts
Queue ->> Worker: Deliver Message
Worker ->> API: Retry Attempt
API --> Worker: ‚úó Failure
Worker ->> Queue: Re-queue
end
Worker ->> Worker: Check: Total attempts > 20<br/>Duration > 24 hours
Note over Worker: Max retries exceeded
Worker ->> DLQ: Move to DLQ<br/>{requestId, failureReason,<br/>totalAttempts: 20,<br/>firstFailure, lastFailure}
DLQ ->> NS: Send Critical Alert Email<br/>"API call permanently failed<br/>after 20 attempts"
NS ->> IMS: Trigger Incident<br/>(PagerDuty/Opsgenie)
IMS ->> Eng: Alert Engineering Team
Note over Eng: Manual intervention required
Eng ->> DLQ: Investigate Failed Message
alt Resolution Found
Eng ->> DLQ: Reprocess Message
DLQ ->> Queue: Re-queue with Fix
else Cannot Resolve
Eng ->> DLQ: Mark as Handled
Note over Eng: Document root cause
end
```
---
## Case 6: Non-Retryable Error (Client Error)
```mermaid
participant "Client Application" as Client
participant "Retry Handler" as RH
participant "API Service" as API
participant "Notification Service" as NS
participant "Logging Service" as Logger
Client ->> RH: Make API Call
RH ->> API: Call API
activate API
API --> RH: ‚úó Failure (400 Bad Request)
deactivate API
Note over RH: Identify as<br/>non-retryable error<br/>(4xx client error)
RH ->> Logger: Log Error Details<br/>{statusCode: 400,<br/>errorType: "CLIENT_ERROR",<br/>noRetry: true}
RH ->> NS: Send Immediate Error Notification<br/>"API call failed with<br/>client error - no retry"
RH --> Client: Return Error Response<br/>(400 Bad Request)
Note over Client,Logger: No queuing - immediate failure<br/>Requires client-side fix
```
---
## Case 7: Circuit Breaker Open
```mermaid
participant "Client Application" as Client
participant "Circuit Breaker" as CB
participant "Retry Handler" as RH
participant "Job Queue" as Queue
participant "Notification Service" as NS
participant "API Service" as API
Client ->> CB: Make API Call
CB ->> CB: Check Circuit State
Note over CB: Circuit: OPEN<br/>(too many recent failures)
CB ->> Queue: Fail Fast - Publish Directly<br/>(skip initial retries)
CB ->> NS: Send Notification<br/>"Circuit open - API call<br/>queued immediately"
CB --> Client: Return Circuit Open Error
Note over CB: Wait for timeout period<br/>(e.g., 30 seconds)
Note over CB: Circuit ‚Üí HALF-OPEN
CB ->> API: Test Request (single)
alt Success
API --> CB: ‚úì Success (200)
Note over CB: Circuit ‚Üí CLOSED
CB ->> NS: Alert: "Circuit closed -<br/>API recovered"
else Failure
API --> CB: ‚úó Failure
Note over CB: Circuit ‚Üí OPEN again
CB ->> NS: Alert: "Circuit remains open"
end
```
---
## Complete Integrated Flow with All Components
```mermaid
participant "Client Application" as Client
participant "Circuit Breaker" as CB
participant "Retry Handler" as RH
participant "API Service" as API
participant "Job Queue (RabbitMQ/SQS)" as Queue
participant "Retry Worker" as Worker
participant "Dead Letter Queue" as DLQ
participant "Notification Service" as NS
participant "Monitoring System" as Monitor
Client ->> CB: Make API Call
CB ->> CB: Check Circuit State
alt Circuit CLOSED
CB ->> RH: Forward Request
RH ->> API: Attempt 1
alt Success
API --> RH: ‚úì Success (200)
RH --> Client: Return Success
RH ->> Monitor: Log Success Metric
else Retryable Failure
API --> RH: ‚úó Failure (5xx)
loop Immediate Retries (3x)
Note over RH: Exponential Backoff
RH ->> API: Retry Attempt
alt Success
API --> RH: ‚úì Success
RH --> Client: Return Success
RH ->> NS: Recovery Alert
else Still Failing
API --> RH: ‚úó Failure
end
end
opt All Immediate Retries Failed
RH ->> Queue: Publish Message<br/>{requestId, attempt: 3,<br/>delay: 3min}
RH ->> NS: Email: "Queued for retry"
RH --> Client: Return Queued Status
Note over Queue: Wait 3 minutes
Queue ->> Worker: Deliver Message
loop Worker Retries (3x)
Worker ->> API: Retry Attempt
alt Success
API --> Worker: ‚úì Success
Worker ->> Queue: Remove Message
Worker ->> NS: Success Alert
else Still Failing
API --> Worker: ‚úó Failure
end
end
opt Still Failing After Worker Retries
Worker ->> Queue: Republish (2hr delay)
Worker ->> NS: Email: "Next retry in 2 hours"
loop Long-term Retries
Note over Queue: Wait 2 hours
Queue ->> Worker: Deliver Message
Worker ->> API: Retry Attempt
alt Success
API --> Worker: ‚úì Success
Worker ->> NS: Final Success Alert
else Max Retries Exceeded
Worker ->> Worker: Check retry limit (20 attempts)
Worker ->> DLQ: Move to DLQ
DLQ ->> NS: Critical Alert
DLQ ->> Monitor: Trigger Incident
end
end
end
end
else Non-Retryable Failure
API --> RH: ‚úó Failure (4xx)
RH ->> Monitor: Log Client Error
RH ->> NS: Immediate Error Alert
RH --> Client: Return Error
end
else Circuit OPEN
CB ->> Queue: Queue Immediately<br/>(fail fast)
CB ->> NS: Circuit Open Alert
CB --> Client: Return Circuit Open Error
Note over CB: After timeout
Note over CB: Circuit ‚Üí HALF-OPEN
CB ->> API: Test Request
alt Success
API --> CB: ‚úì Success
Note over CB: Circuit ‚Üí CLOSED
else Failure
API --> CB: ‚úó Failure
Note over CB: Circuit ‚Üí OPEN
end
end
Note over Monitor: Continuous Monitoring:<br/>‚Ä¢ Queue depth<br/>‚Ä¢ Processing rate<br/>‚Ä¢ Success rate<br/>‚Ä¢ DLQ size<br/>‚Ä¢ Circuit breaker state
```
---
## Queue-Specific Implementation Patterns
### RabbitMQ Pattern
```mermaid
participant "Publisher" as Publisher
participant "Main Exchange" as Exchange
participant "Retry Queue (TTL: 3min)" as Queue
participant "Dead Letter Exchange" as DLX
participant "Retry Queue (TTL: 2hr)" as RetryQ
participant "Dead Letter Queue" as DLQ
participant "Consumer" as Consumer
Publisher ->> Exchange: Publish Message
Exchange ->> Queue: Route to Retry Queue
Note over Queue: Message TTL expires (3min)
Queue ->> DLX: Expired message
DLX ->> Consumer: Deliver for retry
alt Success
Consumer ->> Consumer: Process Successfully
else Failure
Consumer ->> RetryQ: Republish (TTL: 2hr)
Note over RetryQ: Message TTL expires (2hr)
RetryQ ->> DLX: Expired message
DLX ->> Consumer: Deliver for retry
alt Still Failing
Consumer ->> Consumer: Check retry count
opt Max retries exceeded
Consumer ->> DLQ: Move to DLQ
end
end
end
```
### AWS SQS Pattern
```mermaid
participant "Producer" as Producer
participant "Main Queue" as MainQ
participant "Consumer" as Consumer
participant "Retry Queue<br/>(visibility: 3min)" as RetryQ
participant "Dead Letter Queue" as DLQ
Producer ->> MainQ: Send Message
MainQ ->> Consumer: Receive Message
Consumer ->> Consumer: Process Message
alt Success
Consumer ->> MainQ: Delete Message
else Failure
Note over Consumer: Don't delete message
Note over MainQ: Visibility timeout expires
MainQ ->> Consumer: Redeliver Message
Consumer ->> Consumer: Increment retry count
alt Retry limit not reached
Consumer ->> RetryQ: Send to Retry Queue<br/>(DelaySeconds or visibility)
Consumer ->> MainQ: Delete from Main Queue
Note over RetryQ: Wait for visibility timeout
RetryQ ->> Consumer: Receive Message
else Max retries exceeded
Consumer ->> DLQ: Send to DLQ
Consumer ->> MainQ: Delete from Main Queue
end
end
```
### GCP Pub/Sub Pattern
```mermaid
participant "Publisher" as Publisher
participant "Main Topic" as Topic
participant "Subscription<br/>(ack deadline: 60s)" as Sub
participant "Subscriber" as Subscriber
participant "Retry Topic" as RetryTopic
participant "Dead Letter Topic" as DLQ
Publisher ->> Topic: Publish Message
Topic ->> Sub: Push to Subscription
Sub ->> Subscriber: Deliver Message
Subscriber ->> Subscriber: Process Message
alt Success
Subscriber ->> Sub: Acknowledge Message
else Failure
Note over Subscriber: Don't acknowledge
Note over Sub: Ack deadline expires
Sub ->> Subscriber: Redeliver (exponential backoff)
alt Still Failing
Subscriber ->> Subscriber: Check delivery attempts
opt Max attempts exceeded
Sub ->> DLQ: Forward to Dead Letter Topic
Note over DLQ: Manual intervention required
end
else Success
Subscriber ->> Sub: Acknowledge Message
end
end
```
---
## Monitoring Dashboard Flow
```mermaid
participant "System Components" as Sys
participant "Metrics Collector" as Metrics
participant "Monitoring Dashboard" as Dashboard
participant "Alert Manager" as Alert
participant "On-Call Engineer" as Oncall
loop Every 10 seconds
Sys ->> Metrics: Send Metrics<br/>‚Ä¢ Queue depth<br/>‚Ä¢ Processing rate<br/>‚Ä¢ Error rate<br/>‚Ä¢ Circuit breaker state
end
Metrics ->> Dashboard: Update Real-time Dashboard
Metrics ->> Alert: Evaluate Alert Rules
alt Threshold Exceeded
Alert ->> Alert: Check:<br/>‚Ä¢ Queue depth > 1000<br/>‚Ä¢ DLQ size > 10<br/>‚Ä¢ Error rate > 50%<br/>‚Ä¢ Circuit open > 5min
Alert ->> Oncall: Send Alert<br/>(PagerDuty/Slack)
Oncall ->> Dashboard: View Dashboard
Oncall ->> Sys: Investigate & Remediate
Sys ->> Metrics: Metrics improve
Alert ->> Oncall: Send Resolution Alert
end
```
---
## Error Classification Decision Tree
```mermaid
@startuml
start
:API Call Failed;

if (Check HTTP\nStatus Code?) then (5xx)
  :Server Error;
  :Retryable?;
elseif (4xx) then
  :Client Error;
  :Non-Retryable;
  :Log Error;
  :Notify User;
  stop
elseif (Timeout) then
  :Network Error;
  :Retryable?;
elseif (Connection Refused) then
  :Connection Error;
  :Retryable?;
endif

if (Retryable?) then (503, 504, 502)
  :Immediate Retry\nwith Backoff;
elseif (500, 429) then
  if (Has\nRetry-After\nHeader?) then (Yes)
    :Wait for\nRetry-After Duration;
  else (No)
    :Immediate Retry\nwith Backoff;
  endif
endif

repeat
  if (Attempts < 3?) then (Yes)
    :Exponential Backoff\n& Retry;
    if (Success?) then (Yes)
      :Complete Successfully;
      stop
    else (No)
    endif
  else (No)
    :Publish to\nJob Queue;
    :Worker Retry Process;
    repeat
      if (Success after\nlong-term retries?) then (Yes)
        :Complete Successfully;
        stop
      else (No)
        if (Max retries\nexceeded?) then (Yes)
          :Move to DLQ;
          :Manual Intervention;
          stop
        else (No)
          :Continue 2hr\nRetry Cycle;
        endif
      endif
    repeat while
  endif
repeat while

@enduml
graph TD
Start[API Call Failed] --> CheckStatus{Check HTTP<br/>Status Code}
CheckStatus -->|5xx| ServerError[Server Error]
CheckStatus -->|4xx| ClientError[Client Error]
CheckStatus -->|Timeout| NetworkError[Network Error]
CheckStatus -->|Connection Refused| ConnectionError[Connection Error]
ServerError --> Retryable{Retryable?}
ClientError --> NonRetryable[Non-Retryable]
NetworkError --> Retryable
ConnectionError --> Retryable
Retryable -->|503, 504, 502| ImmediateRetry[Immediate Retry<br/>with Backoff]
Retryable -->|500, 429| CheckRetryAfter{Has<br/>Retry-After<br/>Header?}
CheckRetryAfter -->|Yes| RespectHeader[Wait for<br/>Retry-After Duration]
CheckRetryAfter -->|No| ImmediateRetry
NonRetryable --> LogError[Log Error]
NonRetryable --> NotifyUser[Notify User]
NonRetryable --> End[End - No Retry]
ImmediateRetry --> CheckAttempts{Attempts < 3?}
RespectHeader --> CheckAttempts
CheckAttempts -->|Yes| BackoffRetry[Exponential Backoff<br/>& Retry]
CheckAttempts -->|No| QueueJob[Publish to<br/>Job Queue]
BackoffRetry --> Success{Success?}
QueueJob --> WorkerRetry[Worker Retry Process]
Success -->|Yes| Complete[Complete Successfully]
Success -->|No| CheckAttempts
WorkerRetry --> LongRetry{Success after<br/>long-term retries?}
LongRetry -->|Yes| Complete
LongRetry -->|No| CheckMaxRetries{Max retries<br/>exceeded?}
CheckMaxRetries -->|Yes| MoveToDLQ[Move to DLQ]
CheckMaxRetries -->|No| ContinueRetry[Continue 2hr<br/>Retry Cycle]
ContinueRetry --> WorkerRetry
MoveToDLQ --> ManualIntervention[Manual Intervention]
```
@enduml

@startuml
!define RETRY_INDEX_VAR retry_index = 0
!define MAX_IMMEDIATE_RETRIES 3
!define WORKER_RETRY_INTERVAL_HOURS 2
!define MAX_RETRY_DURATION_HOURS 24

start
:Initialize Variables\nretry_index = 0\nstart_time = current_timestamp\nelapsed_time = 0;
:API Call Failed;
:Log Initial Failure\n(Timestamp: {timestamp}\nEndpoint: {endpoint}\nRequest ID: {request_id}\nRetry Index: 0\nStart Time: {start_time});

if (Check HTTP\nStatus Code?) then (5xx)
  :Server Error;
  :Log Server Error\n(Retry Index: {retry_index}\nStatus Code: {status}\nResponse Body: {body}\nServer Headers: {headers}\nElapsed Time: {elapsed_time}h);
  :Retryable?;
  
elseif (4xx) then
  :Client Error;
  if (Error Type?) then (400 Bad Request)
    :Log Validation Error\n(Retry Index: {retry_index}\nInvalid Fields: {fields}\nRequest Payload: {payload}\nValidation Messages: {messages}\nElapsed Time: {elapsed_time}h);
    
  elseif (401 Unauthorized) then
    :Log Auth Error\n(Retry Index: {retry_index}\nToken Status: {token_status}\nAuth Method: {auth_method}\nExpiry Time: {expiry}\nElapsed Time: {elapsed_time}h);
    
  elseif (403 Forbidden) then
    :Log Permission Error\n(Retry Index: {retry_index}\nResource: {resource}\nRequired Permissions: {permissions}\nUser Context: {user}\nElapsed Time: {elapsed_time}h);
    
  elseif (404 Not Found) then
    :Log Resource Error\n(Retry Index: {retry_index}\nRequested Resource: {resource}\nEndpoint: {endpoint}\nParameters: {params}\nElapsed Time: {elapsed_time}h);
    
  elseif (409 Conflict) then
    :Log Conflict Error\n(Retry Index: {retry_index}\nConflicting Data: {conflict_data}\nCurrent State: {current_state}\nRequested State: {requested_state}\nElapsed Time: {elapsed_time}h);
    
  elseif (422 Unprocessable) then
    :Log Data Format Error\n(Retry Index: {retry_index}\nSchema Validation Errors: {errors}\nData Structure: {structure}\nExpected Format: {format}\nElapsed Time: {elapsed_time}h);
    
  else (Other 4xx)
    :Log Generic Client Error\n(Retry Index: {retry_index}\nStatus Code: {status}\nError Message: {message}\nRequest Details: {details}\nElapsed Time: {elapsed_time}h);
  endif
  
  :Non-Retryable;
  :Store in Error Database\n(Permanent Record with retry_index);
  :Notify User;
  stop
  
elseif (Timeout) then
  :Network Error;
  :Log Timeout\n(Retry Index: {retry_index}\nDuration: {duration}\nEndpoint: {endpoint}\nNetwork Conditions: {conditions}\nElapsed Time: {elapsed_time}h);
  :Retryable?;
  
elseif (Connection Refused) then
  :Connection Error;
  :Log Connection Failure\n(Retry Index: {retry_index}\nHost: {host}\nPort: {port}\nDNS Resolution: {dns}\nNetwork Route: {route}\nElapsed Time: {elapsed_time}h);
  :Retryable?;
  
elseif (Data Parsing Error) then
  :Log Parse Error\n(Retry Index: {retry_index}\nRaw Response: {raw_response}\nExpected Format: {format}\nParse Exception: {exception}\nCharacter Position: {position}\nElapsed Time: {elapsed_time}h);
  :Store Raw Response\n(For Analysis with retry_index);
  :Non-Retryable;
  :Send Alert to Technical Team\n**DATA PARSING ERROR - NON-RETRYABLE**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nParse Exception: {exception}\nElapsed Time: {elapsed_time}h\nAction: IMMEDIATE REVIEW REQUIRED;
  stop
  
elseif (Data Validation Error) then
  :Log Validation Error\n(Retry Index: {retry_index}\nFailed Rules: {rules}\nData Sample: {data}\nSchema Version: {schema_version}\nBusiness Rules: {business_rules}\nElapsed Time: {elapsed_time}h);
  :Store Invalid Data\n(Quarantine Database with retry_index);
  :Non-Retryable;
  :Send Alert to Technical Team\n**DATA VALIDATION ERROR - NON-RETRYABLE**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nFailed Rules: {rules}\nElapsed Time: {elapsed_time}h\nAction: IMMEDIATE REVIEW REQUIRED;
  stop
endif

if (Retryable?) then (503, 504, 502)
  :retry_index++;
  :Immediate Retry\nwith Backoff;
  :Log Retry Attempt\n(Retry Index: {retry_index}\nBackoff Duration: {backoff}s\nNext Attempt Time: {next_time}\nElapsed Time: {elapsed_time}h);
  
elseif (500, 429) then
  :retry_index++;
  if (Has\nRetry-After\nHeader?) then (Yes)
    :Wait for\nRetry-After Duration;
    :Log Retry Schedule\n(Retry Index: {retry_index}\nWait Duration: {wait}\nRetry Time: {retry_time}\nElapsed Time: {elapsed_time}h);
  else (No)
    :Immediate Retry\nwith Backoff;
    :Log Retry Attempt\n(Retry Index: {retry_index}\nBackoff Duration: {backoff}s\nElapsed Time: {elapsed_time}h);
  endif
endif

:immediate_attempts = 0;

repeat
  :immediate_attempts++;
  :Calculate elapsed_time\n= (current_timestamp - start_time) / 3600;
  
  if (elapsed_time >= 24?) then (Yes)
    :Log Max Duration Exceeded\n(Retry Index: {retry_index}\nImmediate Attempts: {immediate_attempts}\nElapsed Time: {elapsed_time}h\nStatus: MAX_DURATION_EXCEEDED);
    
    :Send Alert to Technical Team\n**üö® CRITICAL: 24 HOUR RETRY LIMIT EXCEEDED üö®**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nImmediate Attempts: {immediate_attempts}\nElapsed Time: {elapsed_time} hours\nStart Time: {start_time}\nError Type: {error_type}\nEndpoint: {endpoint}\nRequest ID: {request_id}\n‚ö†Ô∏è IMMEDIATE ESCALATION REQUIRED ‚ö†Ô∏è;
    
    :Move to DLQ;
    :Escalate to On-Call Engineer;
    :Manual Intervention Required;
    stop
  endif
  
  if (immediate_attempts < 3?) then (Yes)
    :Exponential Backoff\n& Retry;
    :retry_index++;
    :Log Retry Execution\n(Retry Index: {retry_index}\nImmediate Attempt: {immediate_attempts}/3\nTimestamp: {timestamp}\nBackoff Calculation: {calc}\nElapsed Time: {elapsed_time}h);
    
    if (Success?) then (Yes)
      :Complete Successfully;
      :Log Success\n(Final Retry Index: {retry_index}\nImmediate Attempts: {immediate_attempts}\nTotal Duration: {duration}\nRecovery Time: {recovery}\nElapsed Time: {elapsed_time}h);
      :Update Metrics\n(Success Rate, Retry Stats);
      stop
    else (No)
      :Log Retry Failure\n(Retry Index: {retry_index}\nImmediate Attempt: {immediate_attempts}/3\nError Details: {error}\nNext Retry Time: {next}\nElapsed Time: {elapsed_time}h);
    endif
  else (No)
    :Send Alert to Technical Team\n**‚ö†Ô∏è FIRST 3 IMMEDIATE RETRIES FAILED ‚ö†Ô∏è**\nSeverity: HIGH\nRetry Index: {retry_index}\nImmediate Attempts: 3 (All Failed)\nElapsed Time: {elapsed_time}h\nError Type: {error_type}\nEndpoint: {endpoint}\nRequest ID: {request_id}\nStatus: Moving to Worker Queue;
    
    :Publish to\nJob Queue;
    :Log Queue Submission\n(Retry Index: {retry_index}\nJob ID: {job_id}\nQueue Name: {queue}\nPayload Hash: {hash}\nPriority: {priority}\nElapsed Time: {elapsed_time}h);
    
    :worker_retry_index = 0;
    :last_alert_time = current_timestamp;
    :Worker Retry Process;
    
    repeat
      :worker_retry_index++;
      :Calculate elapsed_time\n= (current_timestamp - start_time) / 3600;
      :Calculate time_since_last_alert\n= (current_timestamp - last_alert_time) / 3600;
      
      if (elapsed_time >= 24?) then (Yes)
        :Log Max Duration Exceeded\n(Main Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nElapsed Time: {elapsed_time}h\nTotal Retries: {total_retries}\nStatus: MAX_DURATION_EXCEEDED);
        
        :Send Alert to Technical Team\n**üö® CRITICAL: 24 HOUR RETRY LIMIT EXCEEDED üö®**\nSeverity: CRITICAL\nMain Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nTotal Retry Duration: {elapsed_time} hours\nStart Time: {start_time}\nEnd Time: {current_timestamp}\nTotal Retries: {total_retries}\nError Type: {error_type}\nEndpoint: {endpoint}\nJob ID: {job_id}\nRequest ID: {request_id}\nError Summary: {full_error_history}\n‚ö†Ô∏è IMMEDIATE ESCALATION REQUIRED ‚ö†Ô∏è;
        
        :Move to DLQ;
        :Store Complete Context\n(All Logs, Request/Response Data,\nFull 24hr Retry Timeline);
        :Escalate to On-Call Engineer;
        :Send SMS/Phone Alert\nto Technical Lead;
        :Manual Intervention Required;
        stop
      endif
      
      :Log Worker Attempt\n(Worker Retry Index: {worker_retry_index}\nMain Retry Index: {retry_index}\nWorker ID: {worker_id}\nTimestamp: {timestamp}\nElapsed Time: {elapsed_time}h);
      
      if (Success after\nlong-term retries?) then (Yes)
        :Complete Successfully;
        :Log Final Success\n(Main Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nTotal Duration: {elapsed_time}h\nWorker ID: {worker_id}\nRecovery Path: {path});
        :Update Metrics;
        stop
      else (No)
        :Log Worker Failure\n(Worker Retry Index: {worker_retry_index}\nMain Retry Index: {retry_index}\nWorker ID: {worker_id}\nError Details: {error}\nElapsed Time: {elapsed_time}h);
        
        if (time_since_last_alert >= 2?) then (Yes - Send 2hr Alert)
          :Send Alert to Technical Team\n**‚è∞ 2-HOUR RETRY UPDATE**\nSeverity: MEDIUM\nMain Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nElapsed Time: {elapsed_time}h / 24h\nTime Since Last Alert: {time_since_last_alert}h\nWorker ID: {worker_id}\nError Type: {error_type}\nRetry Status: CONTINUING\nNext Retry: in 2 hours\nRemaining Time: {24 - elapsed_time}h;
          
          :last_alert_time = current_timestamp;
        endif
        
        :Continue 2hr\nRetry Cycle;
        :Wait 2 hours;
        :Log Retry Schedule\n(Worker Retry Index: {worker_retry_index}\nMain Retry Index: {retry_index}\nNext Retry Time: {next}\nElapsed Time: {elapsed_time}h\nRemaining Time: {24 - elapsed_time}h);
        
      endif
    repeat while (elapsed_time < 24)
  endif
repeat while (immediate_attempts < 3 AND elapsed_time < 24)

@enduml
What is DLQ?
A safety net for failed operations that can't be processed after multiple retries.
Key Benefits:

‚úÖ No Data Loss - Failed jobs are preserved
‚úÖ Non-Blocking - Main queue keeps flowing
‚úÖ Manual Recovery - Team can investigate and retry
‚úÖ Pattern Detection - Identify systemic issues
‚úÖ Audit Trail - Full history of failures
‚úÖ Graceful Degradation - System remains operational

When Jobs Move to DLQ:

‚ùå Max retry attempts exhausted
‚ùå Non-retryable error detected
‚ùå Job timeout exceeded
‚ùå Manual move by operations team

What You Can Do with DLQ:

üëÄ Inspect failed jobs
üîÑ Retry individual jobs
üîÑ Bulk retry all jobs
üìä Analyze error patterns
üóëÔ∏è Remove invalid jobs
üìà Monitor trends

What is a Dead Letter Queue?
A Dead Letter Queue (DLQ) is a special queue that holds messages or jobs that cannot be processed successfully after multiple retry attempts. It's like a "last resort" holding area for failed operations.
DLQ is essential for production systems - it's the difference between data loss and graceful failure recovery! 
‚úÖ Delayed jobs
‚úÖ Job priorities
‚úÖ Retries
‚úÖ Rate limiting
‚úÖ Progress tracking
‚úÖ Job inspection
‚úÖ Stalled detection
‚úÖ Bull Board UI
‚úÖ Redis Cluster
‚úÖ TypeScript
‚úÖ Active support
‚úÖ Rich features
‚úÖ Production-grade
1. Actively maintained
2. Feature-complete
3. Production-tested at scale
4. Better TypeScript support
5. Rich ecosystem (Bull Board)
6. Future-proof
7. Better for teams
8. Better debugging
9. Better monitoring
10. Industry standard

