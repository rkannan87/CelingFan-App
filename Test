@startuml
# Job Queue-Based Retry Handling - Sequence Diagrams
## Case 1: Successful API Call (Happy Path)
```mermaid
participant "Client Application" as Client
participant "API Service" as API
participant "Logging Service" as Logger
Client ->> API: Make API Call
activate API
API --> Client: ✓ Success (200)
deactivate API
Client ->> Logger: Log Success Metric
Note over Client,Logger: Request completed successfully
```
---
## Case 2: Transient Failure → Retry → Success
```mermaid
participant "Client Application" as Client
participant "Retry Handler" as RH
participant "API Service" as API
participant "Notification Service" as NS
participant "Logging Service" as Logger
Client ->> RH: Make API Call (Attempt 1)
RH ->> API: Call API
activate API
API --> RH: ✗ Failure (503)
deactivate API
Note over RH: Exponential Backoff: 1s
RH ->> API: Retry Attempt 2
activate API
API --> RH: ✗ Failure (502)
deactivate API
Note over RH: Exponential Backoff: 2s
RH ->> API: Retry Attempt 3
activate API
API --> RH: ✓ Success (200)
deactivate API
RH --> Client: Return Success
RH ->> Logger: Log: "Succeeded after 3 attempts"
RH ->> NS: Send Recovery Alert
NS --> RH: Alert Sent
Note over Client,Logger: Recovered from transient failure
```
---
## Case 3: Initial Retries Exhausted → Queue for Later
```mermaid
participant "Client Application" as Client
participant "Retry Handler" as RH
participant "API Service" as API
participant "Job Queue" as Queue
participant "Notification Service" as NS
participant "Retry Worker" as Worker
Client ->> RH: Make API Call (Attempt 1)
RH ->> API: Call API
API --> RH: ✗ Failure (503)
Note over RH: Exponential Backoff
RH ->> API: Attempt 2
API --> RH: ✗ Failure (503)
Note over RH: Exponential Backoff
RH ->> API: Attempt 3
API --> RH: ✗ Failure (503)
Note over RH: Initial retries exhausted
RH ->> Queue: Publish Message<br/>{requestId, payload,<br/>attempt: 3, delay: 3min}
RH ->> NS: Email: "Initial retries failed.<br/>Scheduled for retry in 3 mins"
RH --> Client: Return Queued Status
Note over Queue: Wait 3 minutes<br/>(visibility timeout)
Queue ->> Worker: Deliver Message
Worker ->> API: Attempt 4
API --> Worker: ✗ Failure (504)
Worker ->> API: Attempt 5
API --> Worker: ✗ Failure (504)
Worker ->> API: Attempt 6
API --> Worker: ✗ Failure (504)
Worker ->> Queue: Republish with 2-hour delay<br/>{requestId, attempt: 6,<br/>nextRetry: +2hours}
Worker ->> NS: Email: "Still failing.<br/>Next retry in 2 hours"
Note over Queue,Worker: Continues with 2-hour intervals
```
---
## Case 4: Long-term Retry → Eventually Succeeds
```mermaid
participant "Job Queue" as Queue
participant "Retry Worker" as Worker
participant "API Service" as API
participant "Notification Service" as NS
participant "Logging Service" as Logger
Note over Queue: 2-hour retry cycle begins
Queue ->> Worker: Deliver Message (after 2 hours)
Worker ->> API: Attempt 7
API --> Worker: ✗ Failure
Worker ->> API: Attempt 8
API --> Worker: ✗ Failure
Worker ->> API: Attempt 9
API --> Worker: ✗ Failure
Worker ->> Queue: Re-queue for another 2 hours
Note over Queue: Wait 2 hours
Queue ->> Worker: Deliver Message
Worker ->> API: Attempt 10
API --> Worker: ✗ Failure
Worker ->> API: Attempt 11
API --> Worker: ✗ Failure
Worker ->> API: Attempt 12
activate API
API --> Worker: ✓ Success (200)
deactivate API
Worker ->> Queue: Remove from Queue
Worker ->> NS: Email: "API call succeeded<br/>after 12 attempts over 4+ hours"
Worker ->> Logger: Log Recovery Metrics<br/>{totalAttempts: 12,<br/>duration: 4+ hours}
Note over Worker,Logger: Successfully recovered<br/>after extended retry period
```
---
## Case 5: Permanent Failure → Dead Letter Queue
```mermaid
participant "Job Queue" as Queue
participant "Retry Worker" as Worker
participant "API Service" as API
participant "Dead Letter Queue" as DLQ
participant "Notification Service" as NS
participant "Incident Management" as IMS
participant "Engineering Team" as Eng
Note over Queue: 2-hour retry cycles continue
loop Retry Attempts
Queue ->> Worker: Deliver Message
Worker ->> API: Retry Attempt
API --> Worker: ✗ Failure
Worker ->> Queue: Re-queue
end
Worker ->> Worker: Check: Total attempts > 20<br/>Duration > 24 hours
Note over Worker: Max retries exceeded
Worker ->> DLQ: Move to DLQ<br/>{requestId, failureReason,<br/>totalAttempts: 20,<br/>firstFailure, lastFailure}
DLQ ->> NS: Send Critical Alert Email<br/>"API call permanently failed<br/>after 20 attempts"
NS ->> IMS: Trigger Incident<br/>(PagerDuty/Opsgenie)
IMS ->> Eng: Alert Engineering Team
Note over Eng: Manual intervention required
Eng ->> DLQ: Investigate Failed Message
alt Resolution Found
Eng ->> DLQ: Reprocess Message
DLQ ->> Queue: Re-queue with Fix
else Cannot Resolve
Eng ->> DLQ: Mark as Handled
Note over Eng: Document root cause
end
```
---
## Case 6: Non-Retryable Error (Client Error)
```mermaid
participant "Client Application" as Client
participant "Retry Handler" as RH
participant "API Service" as API
participant "Notification Service" as NS
participant "Logging Service" as Logger
Client ->> RH: Make API Call
RH ->> API: Call API
activate API
API --> RH: ✗ Failure (400 Bad Request)
deactivate API
Note over RH: Identify as<br/>non-retryable error<br/>(4xx client error)
RH ->> Logger: Log Error Details<br/>{statusCode: 400,<br/>errorType: "CLIENT_ERROR",<br/>noRetry: true}
RH ->> NS: Send Immediate Error Notification<br/>"API call failed with<br/>client error - no retry"
RH --> Client: Return Error Response<br/>(400 Bad Request)
Note over Client,Logger: No queuing - immediate failure<br/>Requires client-side fix
```
---
## Case 7: Circuit Breaker Open
```mermaid
participant "Client Application" as Client
participant "Circuit Breaker" as CB
participant "Retry Handler" as RH
participant "Job Queue" as Queue
participant "Notification Service" as NS
participant "API Service" as API
Client ->> CB: Make API Call
CB ->> CB: Check Circuit State
Note over CB: Circuit: OPEN<br/>(too many recent failures)
CB ->> Queue: Fail Fast - Publish Directly<br/>(skip initial retries)
CB ->> NS: Send Notification<br/>"Circuit open - API call<br/>queued immediately"
CB --> Client: Return Circuit Open Error
Note over CB: Wait for timeout period<br/>(e.g., 30 seconds)
Note over CB: Circuit → HALF-OPEN
CB ->> API: Test Request (single)
alt Success
API --> CB: ✓ Success (200)
Note over CB: Circuit → CLOSED
CB ->> NS: Alert: "Circuit closed -<br/>API recovered"
else Failure
API --> CB: ✗ Failure
Note over CB: Circuit → OPEN again
CB ->> NS: Alert: "Circuit remains open"
end
```
---
## Complete Integrated Flow with All Components
```mermaid
participant "Client Application" as Client
participant "Circuit Breaker" as CB
participant "Retry Handler" as RH
participant "API Service" as API
participant "Job Queue (RabbitMQ/SQS)" as Queue
participant "Retry Worker" as Worker
participant "Dead Letter Queue" as DLQ
participant "Notification Service" as NS
participant "Monitoring System" as Monitor
Client ->> CB: Make API Call
CB ->> CB: Check Circuit State
alt Circuit CLOSED
CB ->> RH: Forward Request
RH ->> API: Attempt 1
alt Success
API --> RH: ✓ Success (200)
RH --> Client: Return Success
RH ->> Monitor: Log Success Metric
else Retryable Failure
API --> RH: ✗ Failure (5xx)
loop Immediate Retries (3x)
Note over RH: Exponential Backoff
RH ->> API: Retry Attempt
alt Success
API --> RH: ✓ Success
RH --> Client: Return Success
RH ->> NS: Recovery Alert
else Still Failing
API --> RH: ✗ Failure
end
end
opt All Immediate Retries Failed
RH ->> Queue: Publish Message<br/>{requestId, attempt: 3,<br/>delay: 3min}
RH ->> NS: Email: "Queued for retry"
RH --> Client: Return Queued Status
Note over Queue: Wait 3 minutes
Queue ->> Worker: Deliver Message
loop Worker Retries (3x)
Worker ->> API: Retry Attempt
alt Success
API --> Worker: ✓ Success
Worker ->> Queue: Remove Message
Worker ->> NS: Success Alert
else Still Failing
API --> Worker: ✗ Failure
end
end
opt Still Failing After Worker Retries
Worker ->> Queue: Republish (2hr delay)
Worker ->> NS: Email: "Next retry in 2 hours"
loop Long-term Retries
Note over Queue: Wait 2 hours
Queue ->> Worker: Deliver Message
Worker ->> API: Retry Attempt
alt Success
API --> Worker: ✓ Success
Worker ->> NS: Final Success Alert
else Max Retries Exceeded
Worker ->> Worker: Check retry limit (20 attempts)
Worker ->> DLQ: Move to DLQ
DLQ ->> NS: Critical Alert
DLQ ->> Monitor: Trigger Incident
end
end
end
end
else Non-Retryable Failure
API --> RH: ✗ Failure (4xx)
RH ->> Monitor: Log Client Error
RH ->> NS: Immediate Error Alert
RH --> Client: Return Error
end
else Circuit OPEN
CB ->> Queue: Queue Immediately<br/>(fail fast)
CB ->> NS: Circuit Open Alert
CB --> Client: Return Circuit Open Error
Note over CB: After timeout
Note over CB: Circuit → HALF-OPEN
CB ->> API: Test Request
alt Success
API --> CB: ✓ Success
Note over CB: Circuit → CLOSED
else Failure
API --> CB: ✗ Failure
Note over CB: Circuit → OPEN
end
end
Note over Monitor: Continuous Monitoring:<br/>• Queue depth<br/>• Processing rate<br/>• Success rate<br/>• DLQ size<br/>• Circuit breaker state
```
---
## Queue-Specific Implementation Patterns
### RabbitMQ Pattern
```mermaid
participant "Publisher" as Publisher
participant "Main Exchange" as Exchange
participant "Retry Queue (TTL: 3min)" as Queue
participant "Dead Letter Exchange" as DLX
participant "Retry Queue (TTL: 2hr)" as RetryQ
participant "Dead Letter Queue" as DLQ
participant "Consumer" as Consumer
Publisher ->> Exchange: Publish Message
Exchange ->> Queue: Route to Retry Queue
Note over Queue: Message TTL expires (3min)
Queue ->> DLX: Expired message
DLX ->> Consumer: Deliver for retry
alt Success
Consumer ->> Consumer: Process Successfully
else Failure
Consumer ->> RetryQ: Republish (TTL: 2hr)
Note over RetryQ: Message TTL expires (2hr)
RetryQ ->> DLX: Expired message
DLX ->> Consumer: Deliver for retry
alt Still Failing
Consumer ->> Consumer: Check retry count
opt Max retries exceeded
Consumer ->> DLQ: Move to DLQ
end
end
end
```
### AWS SQS Pattern
```mermaid
participant "Producer" as Producer
participant "Main Queue" as MainQ
participant "Consumer" as Consumer
participant "Retry Queue<br/>(visibility: 3min)" as RetryQ
participant "Dead Letter Queue" as DLQ
Producer ->> MainQ: Send Message
MainQ ->> Consumer: Receive Message
Consumer ->> Consumer: Process Message
alt Success
Consumer ->> MainQ: Delete Message
else Failure
Note over Consumer: Don't delete message
Note over MainQ: Visibility timeout expires
MainQ ->> Consumer: Redeliver Message
Consumer ->> Consumer: Increment retry count
alt Retry limit not reached
Consumer ->> RetryQ: Send to Retry Queue<br/>(DelaySeconds or visibility)
Consumer ->> MainQ: Delete from Main Queue
Note over RetryQ: Wait for visibility timeout
RetryQ ->> Consumer: Receive Message
else Max retries exceeded
Consumer ->> DLQ: Send to DLQ
Consumer ->> MainQ: Delete from Main Queue
end
end
```
### GCP Pub/Sub Pattern
```mermaid
participant "Publisher" as Publisher
participant "Main Topic" as Topic
participant "Subscription<br/>(ack deadline: 60s)" as Sub
participant "Subscriber" as Subscriber
participant "Retry Topic" as RetryTopic
participant "Dead Letter Topic" as DLQ
Publisher ->> Topic: Publish Message
Topic ->> Sub: Push to Subscription
Sub ->> Subscriber: Deliver Message
Subscriber ->> Subscriber: Process Message
alt Success
Subscriber ->> Sub: Acknowledge Message
else Failure
Note over Subscriber: Don't acknowledge
Note over Sub: Ack deadline expires
Sub ->> Subscriber: Redeliver (exponential backoff)
alt Still Failing
Subscriber ->> Subscriber: Check delivery attempts
opt Max attempts exceeded
Sub ->> DLQ: Forward to Dead Letter Topic
Note over DLQ: Manual intervention required
end
else Success
Subscriber ->> Sub: Acknowledge Message
end
end
```
---
## Monitoring Dashboard Flow
```mermaid
participant "System Components" as Sys
participant "Metrics Collector" as Metrics
participant "Monitoring Dashboard" as Dashboard
participant "Alert Manager" as Alert
participant "On-Call Engineer" as Oncall
loop Every 10 seconds
Sys ->> Metrics: Send Metrics<br/>• Queue depth<br/>• Processing rate<br/>• Error rate<br/>• Circuit breaker state
end
Metrics ->> Dashboard: Update Real-time Dashboard
Metrics ->> Alert: Evaluate Alert Rules
alt Threshold Exceeded
Alert ->> Alert: Check:<br/>• Queue depth > 1000<br/>• DLQ size > 10<br/>• Error rate > 50%<br/>• Circuit open > 5min
Alert ->> Oncall: Send Alert<br/>(PagerDuty/Slack)
Oncall ->> Dashboard: View Dashboard
Oncall ->> Sys: Investigate & Remediate
Sys ->> Metrics: Metrics improve
Alert ->> Oncall: Send Resolution Alert
end
```
---
## Error Classification Decision Tree
```mermaid
@startuml
start
:API Call Failed;

if (Check HTTP\nStatus Code?) then (5xx)
  :Server Error;
  :Retryable?;
elseif (4xx) then
  :Client Error;
  :Non-Retryable;
  :Log Error;
  :Notify User;
  stop
elseif (Timeout) then
  :Network Error;
  :Retryable?;
elseif (Connection Refused) then
  :Connection Error;
  :Retryable?;
endif

if (Retryable?) then (503, 504, 502)
  :Immediate Retry\nwith Backoff;
elseif (500, 429) then
  if (Has\nRetry-After\nHeader?) then (Yes)
    :Wait for\nRetry-After Duration;
  else (No)
    :Immediate Retry\nwith Backoff;
  endif
endif

repeat
  if (Attempts < 3?) then (Yes)
    :Exponential Backoff\n& Retry;
    if (Success?) then (Yes)
      :Complete Successfully;
      stop
    else (No)
    endif
  else (No)
    :Publish to\nJob Queue;
    :Worker Retry Process;
    repeat
      if (Success after\nlong-term retries?) then (Yes)
        :Complete Successfully;
        stop
      else (No)
        if (Max retries\nexceeded?) then (Yes)
          :Move to DLQ;
          :Manual Intervention;
          stop
        else (No)
          :Continue 2hr\nRetry Cycle;
        endif
      endif
    repeat while
  endif
repeat while

@enduml
graph TD
Start[API Call Failed] --> CheckStatus{Check HTTP<br/>Status Code}
CheckStatus -->|5xx| ServerError[Server Error]
CheckStatus -->|4xx| ClientError[Client Error]
CheckStatus -->|Timeout| NetworkError[Network Error]
CheckStatus -->|Connection Refused| ConnectionError[Connection Error]
ServerError --> Retryable{Retryable?}
ClientError --> NonRetryable[Non-Retryable]
NetworkError --> Retryable
ConnectionError --> Retryable
Retryable -->|503, 504, 502| ImmediateRetry[Immediate Retry<br/>with Backoff]
Retryable -->|500, 429| CheckRetryAfter{Has<br/>Retry-After<br/>Header?}
CheckRetryAfter -->|Yes| RespectHeader[Wait for<br/>Retry-After Duration]
CheckRetryAfter -->|No| ImmediateRetry
NonRetryable --> LogError[Log Error]
NonRetryable --> NotifyUser[Notify User]
NonRetryable --> End[End - No Retry]
ImmediateRetry --> CheckAttempts{Attempts < 3?}
RespectHeader --> CheckAttempts
CheckAttempts -->|Yes| BackoffRetry[Exponential Backoff<br/>& Retry]
CheckAttempts -->|No| QueueJob[Publish to<br/>Job Queue]
BackoffRetry --> Success{Success?}
QueueJob --> WorkerRetry[Worker Retry Process]
Success -->|Yes| Complete[Complete Successfully]
Success -->|No| CheckAttempts
WorkerRetry --> LongRetry{Success after<br/>long-term retries?}
LongRetry -->|Yes| Complete
LongRetry -->|No| CheckMaxRetries{Max retries<br/>exceeded?}
CheckMaxRetries -->|Yes| MoveToDLQ[Move to DLQ]
CheckMaxRetries -->|No| ContinueRetry[Continue 2hr<br/>Retry Cycle]
ContinueRetry --> WorkerRetry
MoveToDLQ --> ManualIntervention[Manual Intervention]
```
@enduml

@startuml
!define RETRY_INDEX_VAR retry_index = 0
!define MAX_RETRIES 3
!define WORKER_MAX_RETRIES 10

start
:Initialize retry_index = 0;
:API Call Failed;
:Log Initial Failure\n(Timestamp: {timestamp}\nEndpoint: {endpoint}\nRequest ID: {request_id}\nRetry Index: 0);
:Send Alert to Technical Team\n**INITIAL FAILURE**\nSeverity: WARNING\nRetry Index: 0;

if (Check HTTP\nStatus Code?) then (5xx)
  :Server Error;
  :Log Server Error\n(Retry Index: {retry_index}\nStatus Code: {status}\nResponse Body: {body}\nServer Headers: {headers});
  :Send Alert to Technical Team\n**SERVER ERROR - 5xx**\nSeverity: HIGH\nRetry Index: {retry_index}\nStatus: {status_code};
  :Retryable?;
  
elseif (4xx) then
  :Client Error;
  if (Error Type?) then (400 Bad Request)
    :Log Validation Error\n(Retry Index: {retry_index}\nInvalid Fields: {fields}\nRequest Payload: {payload}\nValidation Messages: {messages});
    :Send Alert to Technical Team\n**DATA VALIDATION ERROR - 400**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nInvalid Fields: {fields}\nPayload Sample: {payload_sample};
    
  elseif (401 Unauthorized) then
    :Log Auth Error\n(Retry Index: {retry_index}\nToken Status: {token_status}\nAuth Method: {auth_method}\nExpiry Time: {expiry});
    :Send Alert to Technical Team\n**AUTHENTICATION ERROR - 401**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nAuth Method: {auth_method}\nToken Status: {token_status};
    
  elseif (403 Forbidden) then
    :Log Permission Error\n(Retry Index: {retry_index}\nResource: {resource}\nRequired Permissions: {permissions}\nUser Context: {user});
    :Send Alert to Technical Team\n**PERMISSION ERROR - 403**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nResource: {resource}\nUser: {user_id};
    
  elseif (404 Not Found) then
    :Log Resource Error\n(Retry Index: {retry_index}\nRequested Resource: {resource}\nEndpoint: {endpoint}\nParameters: {params});
    :Send Alert to Technical Team\n**RESOURCE NOT FOUND - 404**\nSeverity: HIGH\nRetry Index: {retry_index}\nResource: {resource}\nEndpoint: {endpoint};
    
  elseif (409 Conflict) then
    :Log Conflict Error\n(Retry Index: {retry_index}\nConflicting Data: {conflict_data}\nCurrent State: {current_state}\nRequested State: {requested_state});
    :Send Alert to Technical Team\n**DATA CONFLICT ERROR - 409**\nSeverity: HIGH\nRetry Index: {retry_index}\nConflict Details: {conflict_summary};
    
  elseif (422 Unprocessable) then
    :Log Data Format Error\n(Retry Index: {retry_index}\nSchema Validation Errors: {errors}\nData Structure: {structure}\nExpected Format: {format});
    :Send Alert to Technical Team\n**DATA FORMAT ERROR - 422**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nSchema Errors: {validation_errors}\nExpected vs Actual: {comparison};
    
  else (Other 4xx)
    :Log Generic Client Error\n(Retry Index: {retry_index}\nStatus Code: {status}\nError Message: {message}\nRequest Details: {details});
    :Send Alert to Technical Team\n**CLIENT ERROR - 4xx**\nSeverity: HIGH\nRetry Index: {retry_index}\nStatus: {status_code};
  endif
  
  :Non-Retryable;
  :Store in Error Database\n(Permanent Record with retry_index);
  :Send FINAL Alert to Technical Team\n**NON-RETRYABLE ERROR**\nSeverity: CRITICAL\nTotal Retry Index: {retry_index}\nAction: MANUAL INTERVENTION REQUIRED;
  :Create Incident Ticket\n(Jira/ServiceNow)\nRetry Index: {retry_index};
  :Notify User;
  stop
  
elseif (Timeout) then
  :Network Error;
  :Log Timeout\n(Retry Index: {retry_index}\nDuration: {duration}\nEndpoint: {endpoint}\nNetwork Conditions: {conditions});
  :Send Alert to Technical Team\n**NETWORK TIMEOUT**\nSeverity: MEDIUM\nRetry Index: {retry_index}\nTimeout Duration: {duration}s;
  :Retryable?;
  
elseif (Connection Refused) then
  :Connection Error;
  :Log Connection Failure\n(Retry Index: {retry_index}\nHost: {host}\nPort: {port}\nDNS Resolution: {dns}\nNetwork Route: {route});
  :Send Alert to Technical Team\n**CONNECTION REFUSED**\nSeverity: HIGH\nRetry Index: {retry_index}\nHost: {host}:{port};
  :Retryable?;
  
elseif (Data Parsing Error) then
  :Log Parse Error\n(Retry Index: {retry_index}\nRaw Response: {raw_response}\nExpected Format: {format}\nParse Exception: {exception}\nCharacter Position: {position});
  :Store Raw Response\n(For Analysis with retry_index);
  :Send Alert to Technical Team\n**DATA PARSING ERROR**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nParse Exception: {exception}\nResponse Sample: {sample}\nAction: CHECK DATA FORMAT;
  :Non-Retryable;
  :Create Incident Ticket;
  stop
  
elseif (Data Validation Error) then
  :Log Validation Error\n(Retry Index: {retry_index}\nFailed Rules: {rules}\nData Sample: {data}\nSchema Version: {schema_version}\nBusiness Rules: {business_rules});
  :Store Invalid Data\n(Quarantine Database with retry_index);
  :Send Alert to Technical Team\n**DATA VALIDATION ERROR**\nSeverity: CRITICAL\nRetry Index: {retry_index}\nFailed Rules: {rules_list}\nData Sample: {data_sample}\nAction: REVIEW DATA QUALITY;
  :Non-Retryable;
  :Create Incident Ticket;
  stop
endif

if (Retryable?) then (503, 504, 502)
  :retry_index++;
  :Immediate Retry\nwith Backoff;
  :Log Retry Attempt\n(Retry Index: {retry_index}\nBackoff Duration: {backoff}s\nNext Attempt Time: {next_time});
  :Send Alert to Technical Team\n**RETRY INITIATED**\nSeverity: INFO\nRetry Index: {retry_index}/{MAX_RETRIES}\nBackoff: {backoff}s;
  
elseif (500, 429) then
  :retry_index++;
  if (Has\nRetry-After\nHeader?) then (Yes)
    :Wait for\nRetry-After Duration;
    :Log Retry Schedule\n(Retry Index: {retry_index}\nWait Duration: {wait}\nRetry Time: {retry_time});
    :Send Alert to Technical Team\n**SCHEDULED RETRY**\nSeverity: INFO\nRetry Index: {retry_index}\nWait Duration: {wait}s;
  else (No)
    :Immediate Retry\nwith Backoff;
    :Log Retry Attempt\n(Retry Index: {retry_index}\nBackoff Duration: {backoff}s);
    :Send Alert to Technical Team\n**RETRY INITIATED**\nSeverity: INFO\nRetry Index: {retry_index}/{MAX_RETRIES};
  endif
endif

repeat
  if (Attempts < 3?) then (Yes)
    :Exponential Backoff\n& Retry;
    :Log Retry Execution\n(Retry Index: {retry_index}\nAttempt: {attempt}/3\nTimestamp: {timestamp}\nBackoff Calculation: {calc});
    :Send Alert to Technical Team\n**EXECUTING RETRY**\nSeverity: INFO\nRetry Index: {retry_index}\nAttempt: {attempt}/3;
    
    if (Success?) then (Yes)
      :Complete Successfully;
      :Log Success\n(Final Retry Index: {retry_index}\nTotal Attempts: {attempts}\nDuration: {duration}\nRecovery Time: {recovery});
      :Send Alert to Technical Team\n**RECOVERED SUCCESSFULLY**\nSeverity: INFO\nRetry Index: {retry_index}\nTotal Attempts: {attempts}\nRecovery Time: {recovery}s;
      :Update Metrics\n(Success Rate, Retry Stats);
      stop
    else (No)
      :retry_index++;
      :Log Retry Failure\n(Retry Index: {retry_index}\nAttempt: {attempt}/3\nError Details: {error}\nNext Retry Time: {next});
      :Send Alert to Technical Team\n**RETRY FAILED**\nSeverity: MEDIUM\nRetry Index: {retry_index}\nAttempt: {attempt}/3\nNext Retry: {next_time};
    endif
  else (No)
    :Publish to\nJob Queue;
    :Log Queue Submission\n(Retry Index: {retry_index}\nJob ID: {job_id}\nQueue Name: {queue}\nPayload Hash: {hash}\nPriority: {priority});
    :Send Alert to Technical Team\n**MOVED TO JOB QUEUE**\nSeverity: MEDIUM\nRetry Index: {retry_index}\nJob ID: {job_id}\nQueue: {queue_name};
    :worker_retry_index = 0;
    :Worker Retry Process;
    
    repeat
      :worker_retry_index++;
      :Log Worker Attempt\n(Worker Retry Index: {worker_retry_index}\nMain Retry Index: {retry_index}\nWorker ID: {worker_id}\nTimestamp: {timestamp});
      :Send Alert to Technical Team\n**WORKER RETRY ATTEMPT**\nSeverity: INFO\nWorker Retry Index: {worker_retry_index}/{WORKER_MAX_RETRIES}\nMain Retry Index: {retry_index}\nWorker ID: {worker_id};
      
      if (Success after\nlong-term retries?) then (Yes)
        :Complete Successfully;
        :Log Final Success\n(Main Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nTotal Duration: {duration}\nWorker ID: {worker_id}\nRecovery Path: {path});
        :Send Alert to Technical Team\n**WORKER RECOVERED SUCCESSFULLY**\nSeverity: INFO\nMain Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nTotal Duration: {duration}s\nWorker ID: {worker_id};
        :Update Metrics;
        stop
      else (No)
        :Log Worker Failure\n(Worker Retry Index: {worker_retry_index}\nMain Retry Index: {retry_index}\nWorker ID: {worker_id}\nError Details: {error});
        :Send Alert to Technical Team\n**WORKER RETRY FAILED**\nSeverity: MEDIUM\nWorker Retry Index: {worker_retry_index}/{WORKER_MAX_RETRIES}\nMain Retry Index: {retry_index}\nError: {error_summary};
        
        if (Max retries\nexceeded?) then (Yes)
          :Move to DLQ;
          :Log DLQ Placement\n(Main Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nJob ID: {job_id}\nFull Error History: {history}\nFinal State: {state});
          :Store Complete Context\n(All Logs, Request/Response Data,\nRetry Timeline with all indices);
          :Send CRITICAL Alert to Technical Team\n**MOVED TO DEAD LETTER QUEUE**\nSeverity: CRITICAL\nMain Retry Index: {retry_index}\nWorker Retry Index: {worker_retry_index}\nTotal Retries: {total_retries}\nJob ID: {job_id}\nError Summary: {summary}\n⚠️ IMMEDIATE ACTION REQUIRED ⚠️;
          :Create P1 Incident Ticket\n(High Priority)\nRetry Indices: {retry_index}, {worker_retry_index};
          :Escalate to On-Call Engineer;
          :Send SMS/Phone Alert\nto Technical Lead;
          :Manual Intervention;
          stop
        else (No)
          :Continue 2hr\nRetry Cycle;
          :Log Retry Schedule\n(Worker Retry Index: {worker_retry_index}\nMain Retry Index: {retry_index}\nNext Retry Time: {next}\nBackoff Duration: {backoff});
          :Send Alert to Technical Team\n**CONTINUING RETRY CYCLE**\nSeverity: INFO\nWorker Retry Index: {worker_retry_index}\nNext Retry: {next_time}\nBackoff: {backoff}s;
        endif
      endif
    repeat while
  endif
repeat while

@enduml
